---
title: "Statistical Analysis, MSCA 31007, Lecture 3"
author: "Hope Foster-Reyes"
date: "October 28, 2016"
output:
  pdf_document:
    toc: true
    toc_depth: 3
geometry: margin=0.75in
---

# Simulation of 4 Linear Models

Understand common assumptions behind the linear model.

_Notes_

* _Style Guide:_ https://google.github.io/styleguide/Rguide.xml

* _Packages required: None_

* _Files required: None_

## Set Up

Set the values of slope (a) and intercept (b) parameters. Set the sample length to 1,000.

```{r setup}
# Slope and Intercept
a <- 0.8; b <- 0.1
sample.n <- 1000

# Seeds
seed1 <- 111
seed2 <- 1112131415

options(scipen = 5)
```

## Introduction

Let's imagine that X is a normally distributed variable, and Y is a variable which is correlated to X with a simple linear relationship. We can simulate this relationship in R. Later we will compare this model to a more complex model exhibiting randomness in Y.

Input variable $X \sim Norm(\mu= 3, \sigma=2.5)$

```{r intro}
# Simulate the X variable using rnorm()
set.seed(seed1)
model1.x.sd <- 2.5
model1.x <- rnorm(sample.n, mean = 3, sd = model1.x.sd)

# Simulate a simple introductory model with no residual 
model.intro.y <- (a * model1.x + b) 
model.intro <- data.frame(Y=model.intro.y, X=model1.x)

title <- "Intro Model"
plot(model.intro$X, model.intro$Y, main = title, ylim = c(-5, 15))
```

## 1. Model 1

Now we'll take the above simple introductory model and add a "Gaussian" (normal) residual representing variation in Y that is undescribed by our simplistic model above. Thus we will create X as our "independent" or "predictor" random variable and Y as our "dependent" or "response" output random variable.

### Explanation

In this model, the value of the random variable Y varies, i.e. it is not a constant. A portion of this variation is *explained* by the variation in X or is *predicted* by X. This portion is described by our earlier introductory model:

$Y = aX + b$

A portion of the variation in Y however is *not explained* by our model. This accounts for the randomness in Y (and, notably, accounts for the reason that we use statistics rather than simple algebra to predict or explain our Y values). 

To represent this, we use the symbol $\varepsilon$. We describe this as following:

**Equation 1.** 

$Y = aX + b + \varepsilon$

$\varepsilon \sim Norm(0, \sigma_{\varepsilon})$

What if we saw that there was randomness in Y, but we did not know or suspect that Y had a relationship with X? We could still represent Y as a random variable with the following:

**Equation 2.** 

$Y = \mu_{Y} + \varepsilon$

$\varepsilon \sim Norm(0, \sigma_{Y})$

Since the above states that "Y is equal to its expected value (mean) plus its normally-distributed standard deviation", it is equivalent to the following:

$Y \sim Norm(\mu_{Y}, \sigma_{Y})$

It is important to note the parallel meaning of $\varepsilon$ in both equations. Both equations represent randomness in Y, and both represent that randomness with the symbol $\varepsilon$. However, the first equation adds an element of "knowing" that is critical when we attempt to utilize statistics to find relationships between variables. 

One could say that a goal of statistical application is to explain the relationship between predictor variables and response variables so that we can "play God" and cause desired events to occur by manually altering our predictors to achieve our desired response.

In order to do so, we need to know **more** about Y than the above Equation 2 tells us, namely that "it is random and seems to center around this number and vary about this much in this way". The "more" that we would like to know (if possible) is how Y responds according to a predictor, or set of predictors. Thus the first equation provides both, the **predictor**, "X", and the **"how"**, our slope and intercept a and b.

Yet in both cases $\varepsilon$ is our "smoke screen" of reality, also known as randomness. One way of looking at our first equation is that it takes a *portion* of the unpredictable in $\varepsilon$ and converts it into a predictable ($aX + b$). We will refer to the randomness or unpredictable element in our models as the "error" or the "residuals". Thus when we refer to $\sigma_{\varepsilon}$ we are referring to the "residual standard deviation".

Naturally, the value of $\sigma$ will differ in our Equation 1 and Equation 2 above, were they to refer to the same random variable Y. The reason they vary is precisely because the predictor variable X "takes away" (i.e. *explains*) some of the variability that our Equation 2 $\varepsilon$ explains. Thus we should find that our $\sigma_{Y}$ is greater than our $\sigma_{\varepsilon}$.

Finally, it is important to note that we are answering the question:

**How does Y respond to predictor X?**

with the answer: **"It responds in a linear fashion with some additional Gaussian randomness."** That is the basis for our use of a linear model here. We will learn that is some cases factors will prevent us from using this model to analyze our data. Understanding the assumptions which lead us to choose a model and a method will become an important part of data analysis.

### Simulation

Simulate and plot Model 1:

Input variable $X \sim Norm(\mu= 3, \sigma=2.5)$; model residuals $Eps \sim Norm(\mu = 0, \sigma = 1.5)$

```{r m1}
# Simulate the Eps residual using rnorm()
set.seed(seed2)
model1.eps.sd <- 1.5
model1.eps <- rnorm(sample.n, mean = 0, sd = model1.eps.sd)

model1.y <- (a * model1.x + b) + model1.eps
model1 <- data.frame(Y=model1.y, X=model1.x, Eps=model1.eps)

# Inspect results and compare with our simplistic model
title <- "Intro Model"
plot(model.intro$X, model.intro$Y, main = title, ylim = c(-5, 15))

title <- "Model 1"
head(model1)
plot(model1$X, model1$Y, main = title)
plot(model1$Eps, type = "l", main = paste(title, "Residuals"))
```

Taking a deeper look at our residuals from different perspectives:

```{r m1-plot, fig.width=5.2, fig.height=3.6}
plot(model1$Eps, main = paste(title, "Residuals"))
boxplot(model1$Eps, main = paste(title, "Residuals"))
table(cut(model1$Eps, breaks = nclass.Sturges(model1$Eps)))
hist(model1$Eps, main = paste(title, "Residuals"))

# Theoretical and sample density
model1.xaxis <- (100 * floor(min(model1.eps))) : (100 * ceiling(max(model1.eps)))
model1.xaxis <- model1.xaxis / 100
plot(model1.xaxis, dnorm(model1.xaxis, mean = 0, sd = model1.eps.sd), 
     type = "l", ylim = c(0, 0.3), col = "black", ylab = "Density", 
     main = paste(title, "Theoretical and Sample Eps"))
lines(density(model1.eps), col = "Red")
```

### Exploration

Let's explore our model sample a bit to confirm some of our expectations. Bearing in mind below that the mean of any constant is itself, and the variance of any constant is 0:

```{r m1-explore}
# Our Y values are indeed calculated according to our model equation
(a * model1.x[0:5] + b + model1.eps[0:5])
(model1.y[0:5])

# The mean of our Epsilon sample is ~ 0, hence the mean of aX + b is ~ equivalent to the 
#   mean of Y, or a * the mean of X + b.
(mean(model1.eps))
c(mean((a * model1.x) + b), a * mean(model1.x) + b, mean(model1.y))

# The standard deviation of the sample Y is greater than the standrd deviation of Epsilon
c(sd(model1.y), sd(model1.eps))

# The variance of the sample Y is equal to the variance of aX + Epsilon
c(var(model1.y), var(a * model1.x + model1.eps))

# The standard deviation of the sample Y is equal to the sd of aX + Epsilon
c(sd(model1.y), sd(a * model1.x + model1.eps))
```

The above properties will be relevant to our use below of the Method of Moments to estimate parameters.

## 2. Model 2

Simulate and plot Model 2:

Input variable $X \sim Norm(\mu= 3, \sigma=2.5)$; model residuals $Eps \sim Unif(min=-4.33, max=4.33)$

```{r m2}
# Simulate the model using X from Model 1 and runif() for Eps
set.seed(seed2)
model2.min <- -4.33; model2.max <- 4.33
model2.eps <- runif(sample.n, min = model2.min, max = model2.max)

model2.y <- (a * model1.x + b) + model2.eps
model2 <- data.frame(Y=model2.y, X=model1.x, Eps=model2.eps)

# Inspect results
title <- "Model 2"
head(model2)
plot(model2$X, model2$Y, main = title)
plot(model2$Eps, type = "l", main = paste(title, "Residuals"))
```

Taking a deeper look at our residuals from different perspectives:

```{r m2-plot, fig.width=5.2, fig.height=3.6}
plot(model2$Eps, main = paste(title, "Residuals"))
boxplot(model2$Eps, main = paste(title, "Residuals"))
table(cut(model2$Eps, breaks = nclass.Sturges(model2$Eps)))
hist(model2$Eps, main = paste(title, "Residuals"))

# Theoretical and sample density
model2.xaxis <- (100 * floor(min(model2.eps))) : (100 * ceiling(max(model2.eps)))
model2.xaxis <- model2.xaxis / 100
plot(model2.xaxis, dunif(model2.xaxis, model2.min, model2.max), 
     type = "l", ylim = c(0, 0.3), col = "black", ylab = "Density", 
     main = paste(title, "Theoretical and Sample Eps"))
lines(density(model2.eps), col = "Red")
```

## 3. Model 3

Simulate and plot Model 3:

Input variable $X \sim Norm(\mu= 3, \sigma=2.5)$; model residuals $Eps \sim Cauchy*(location=0, scale=3)$

```{r m3}
# Simulate the model using X from Model 1 and rcauchy() for Eps
set.seed(seed2)
model3.eps <- rcauchy(sample.n, location = 0, scale = 0.3)

model3.y <- (a * model1.x + b) + model3.eps
model3 <- data.frame(Y=model3.y, X=model1.x, Eps=model3.eps)

# Inspect results
title <- "Model 3"
head(model3)
plot(model3$X, model3$Y, main = title)
plot(model3$Eps, type = "l", main = paste(title, "Residuals"))
```

Here it appears that we've lost our correlation entirely due to the shape appearing with no slope in our plot of Model 3. Let's take a closer look by plotting the model with the same scale as our previous models. We'll lose several of our Y outliers from the display, but we'll get a closer look at the center of our Y values.

Just to keep our bearings, we'll compare this to our original simplistic model with no residuals.

```{r m3-zoom}
plot(model3$X, model3$Y, main = paste(title, "Zoomed In"), 
     ylim = c(-5, 15))
title <- "Intro Model"
plot(model.intro$X, model.intro$Y, main = title, ylim = c(-5, 15))
```

We'll also take a deeper look at our residuals from different perspectives. Note how the scale of the following plots radically differs from our earlier residual plats.

```{r m3-plot, fig.width=5.2, fig.height=3.6}
title <- "Model 3"

plot(model3$Eps, main = paste(title, "Residuals"))
boxplot(model3$Eps, main = paste(title, "Residuals"))
table(cut(model3$Eps, breaks = nclass.Sturges(model3$Eps)))
hist(model3$Eps, main = paste(title, "Residuals"))

# Sample density
plot(density(model3.eps), main = paste(title, "Residual Density"))
```

Estimate the standard deviation of the residuals.

```{r m3-sd}
(sd(model3$Eps))
```

Generate another 5 samples of residuals without any seed specification and estimate standard deviations for each of them.

```{r m3-residuals}
model3.eps1 <- rcauchy(sample.n, location = 0, scale = 0.3)
model3.eps2 <- rcauchy(sample.n, location = 0, scale = 0.3)
model3.eps3 <- rcauchy(sample.n, location = 0, scale = 0.3)
model3.eps4 <- rcauchy(sample.n, location = 0, scale = 0.3)
model3.eps5 <- rcauchy(sample.n, location = 0, scale = 0.3)
c(sd(model3.eps1), sd(model3.eps2), sd(model3.eps3), sd(model3.eps4), sd(model3.eps5))
```

Note the irregularity of standard deviations from sample to sample.

***How do you interpret this observation?***

The Cauchy distribution is unique in that its expected value, i.e. mean, and other moments of any order n >= 1 do not exist. For example, in contrast with a Cauchy, for the normal distribution we can define parameters which will result in theoretical moments. For any adequately large sample based on this specific distribution (i.e. these specific parameters), the sample will demonstrate moments (which may be calculated using its moment generating function) which are close to the theoretical moments, and which, as the sample becomes larger and larger, will converge on those moments.

Unlike the normal distribution and other typical families of distributions, the Cauchy family of distributions does not demonstrate this same quality. There is no moment generating function and therefore no theoretical moments. While we may still calculate a number for the mean or standard deviation of an individual sample using the common formulas, these numbers will not bear any consistency or resemblance to any theoretical mean or standard deviation, as for Cauchy distributions these do not exist. 

Similarly, generating multiple samples over time will not demonstrate consistency between the mean and standard deviations of the individual samples, nor demonstrate a normal sampling distribution. Thus the qualities of a sampling distribution taken from a Cauchy distribution is an exception to the Central Limit Theorem.

## 4. Model 4

Simulate and plot Model 4:

Input variable $X \sim Norm(\mu= 3, \sigma=2.5)$; model residuals $Eps \sim$ a heteroscedastic process.

Heteroscedasticity means that even though the model residuals Eps may be generated by a normal distribution, the standard deviation parameters for different sub samples are different.

Create the process of standard deviations in which the first 50 observations have sigma=2, followed by 75 observations with sigma=3.4, followed by 75 observations with sigma=0.8 and concluded by 50 observations with sigma=2.6.

Plot the trajectory of standard deviations of total length nSample=1000.

```{r m4-residuals}
# Create heteroscedastic process
hetero.sd <- c(2, 3.4, 0.8, 2.6)
hetero.loop <- c(rep(hetero.sd[1], 50),
                 rep(hetero.sd[2], 75),
                 rep(hetero.sd[3], 75),
                 rep(hetero.sd[4], 50))
hetero.process <- rep(hetero.loop, 4)
plot(hetero.process, type = "l")

# Simulate the linear model residuals Eps with changing standard deviations.
set.seed(seed2)
model4.eps <- rnorm(sample.n) * hetero.process

# Plot the residuals
title <- "Model 4"
plot(model4.eps, type = "l", main = paste(title, "Residuals"))
```

Taking a deeper look at our residuals from different perspectives:

```{r m4-plot, fig.width=5.2, fig.height=3.6}
plot(model4.eps, main = paste(title, "Residuals"))
boxplot(model4.eps, main = paste(title, "Residuals"))
table(cut(model4.eps, breaks = nclass.Sturges(model4.eps)))
hist(model4.eps, main = paste(title, "Residuals"))
```

Observe how heteroscedasticity transforms normal distribution into leptokurtic distribution.

```{r m4-demo}
hetero.x <- (100 * floor(min(model4.eps))) : (100 * ceiling(max(model4.eps)))
hetero.x <- hetero.x / 100

# Plot the theoretical distribution
plot(hetero.x, dnorm(hetero.x, mean = mean(model4.eps), sd = sd(model4.eps)), 
     type = "l", ylim = c(0, 0.3), col = "black", 
     ylab = "Distribution of Heteroscedastic Eps")

# And the heteroscedastic sample distribution
lines(density(model4.eps), col = "Red")
```

Generate Model 4 and plot. Plot our Model 1 again with the same scale to observe the differences side by side.

```{r m4}
# Simulate the model using X from Model 1 and heteroscedastic residuals
model4.y <- (a * model1.x + b) + model4.eps
model4 <- data.frame(Y=model4.y, X=model1.x, Eps=model4.eps)

# Inspect results
plot(model4$X, model4$Y, main = title)
plot(model1$X, model1$Y, main = "Model 1", ylim = c(-9, 18))
```

## 5. Effect of Residual Distribution on Correlation

Calculate the theoretical $\rho^{2}$ for the “correct model” which is Model 1.

```{r q5-theoretical}
# Calculate squared correlation coefficient using the parameters specified 
#   for Model 1 X and Eps
(rho.sq.theoretical <- (a * model1.x.sd)^2 / ((a * model1.x.sd)^2 + model1.eps.sd^2))
```

And compare with the estimated $\rho^{2}$

```{r q5-simulated}
c(cor(model1$X, model1$Y)^2,
  cor(model2$X, model2$Y)^2,
  cor(model3$X, model3$Y)^2,
  cor(model4$X, model4$Y)^2)
```

***How do you interpret the results?***

*For Model 1:*

Our correlation is close to the theoretical distribution, predictably since it is a simulated sample from that model.

*For Model 2:*

We still see a strong correlation but it is weaker than Model 1. This we can also expect since the residual is "more spread out". The residual from Model one was based on a normal distribution, therefore the residual values, and resultant Y values, were largely packed very close to the correlated x,y line, as in a normal distribution most of the values (the top of the bell curve) are close to the mean. 

A unified distribution, however, does not have most of its values close to the mean, instead its values are evenly distributed. Therefore when our residual distribution was based on a unified distribution, the resultant Y values are evenly distributed away from the x,y line but still remain as close as the interval we specified, limiting their distance away.

*For Model 3:*

Comparing the Model 3 plot with the Model 3 zoomed-in plot above, we can see that the central Y values still have a strong relationship with X, yet the Y outliers are so extreme that a chart displaying all Y values seems to entirely obfuscate the relationship. This is caused by the residual's Cauchy distribution whose outliers radically alter the Y values when added to the model.

The impact of these extreme values is so intense as to reduce the calculated correlation dramatically from and expected 0.64 to ~0.0092.

*For Model 4:*

Again, let's compare the Model 4 plot, this time with our Model 1 plot. The residuals for both these plots utilize a normal distribution, however the Model 4 residuals have a heterogenous standard deviation produced by a process which generated residuals using a different standard deviation for multiple subsets of our 1000 simulated values.

We see that the correlation for this sample is lower, why? Recall that our original Model 1 set of residuals was based on a standard deviation of 1.5 and our heteroscedastic set is based on a standard deviation that fluctuates between 2, 3.4, 0.8, and 2.6. 

We can actually calculate the mean standard deviation of our heteroscendastic set by calculating the mean of the process we created. It is `r mean(hetero.process)`. As 2.18 is a larger deviation than 1.5, we are relieved to see that the correlation is also lower as expected.

What if we were to create a heteroscedastic process of standard deviations with a mean closer to 1.5?

```{r q5-m5, fig.width=5.2, fig.height=3.6}
# Create heteroscedastic process
hetero.sd <- c(1.75, 1.6, 1.1, 1.95)
hetero.loop <- c(rep(hetero.sd[1], 50),
                 rep(hetero.sd[2], 75),
                 rep(hetero.sd[3], 75),
                 rep(hetero.sd[4], 50))
hetero.process <- rep(hetero.loop, 4)
plot(hetero.process, type = "l")

# Simulate the linear model residuals Eps with changing standard deviations.
set.seed(seed2)
model5.eps <- rnorm(sample.n) * hetero.process

# Simulate the model using X from Model 1 and heteroscedastic residuals
model5.y <- (a * model1.x + b) + model5.eps
model5 <- data.frame(Y=model5.y, X=model1.x, Eps=model5.eps)

# Inspect results
title = "Model 5"
plot(model5$X, model5$Y, main = title, ylim = c(-10, 20))

# Observe the distribution of the residuals
hetero.x <- (100 * floor(min(model5.eps))) : (100 * ceiling(max(model5.eps)))
hetero.x <- hetero.x / 100

# Plot the theoretical distribution
plot(hetero.x, dnorm(hetero.x, mean = mean(model5.eps), sd = sd(model5.eps)), 
     type = "l", ylim = c(0, 0.3), col = "black", 
     ylab = "Distribution of Heteroscedastic Eps")

# And the heteroscedastic sample distribution
lines(density(model5.eps), col = "Red")

# Calculate correlation
cor(model5$X, model5$Y)^2
```

Once again we are reassured that the correlation is now even closer to our theoretical 0.64.

Heteroscedasticity in this second example does transform the residual distribution intoa more  leptokurtic shape, but the result is less extreme.

## 6. Estimation of Linear Model

Estimate parameters $a, b, \sigma$ using the function lm(). Explore its output.

Recall that the $\sigma$ we refer to here is the "residual standard deviation" or the square root of the estimated variance of our Epsilon "error" (randomness).

```{r q6}
model1.est <- lm(Y~X, data = model1)
(model1.summary <- summary(model1.est))

names(model1.summary)
model1.summary$r.squared
model1.summary$coefficients
model1.summary$sigma^2
var(model1.summary$residuals)
```

Note the difference between `model1.summary$sigma^2` and `var(model1.summary$residuals)`.

The estimate returned by analyzing the model itself finds $\sigma$ using 2 degrees of freedom (df), or N-2 in the denominator. But `var()` does not know anything about the model and its df. It estimates variance using N-1 in the denominator.

The two estimates are reconciled by $\frac{N-1}{N-2}$.

```{r q6-sigma}
var(model1.summary$residuals) * (sample.n - 1) / (sample.n - 2)
```

### Estimation with Method of Moments

***Estimate the same parameters using the method of moments directly.***

The method of moments states to first define a set of moments of our population (the model) as functions of our unknown parameters which define the model.

#### 1. Therefore we can define:

$Y = aX + b + \varepsilon$

$Eps \sim Norm(\mu = 0, \sigma = 1.5)$

Expectation of Y, i.e. first moment:

$E[Y] = E[aX + b + \varepsilon]$

$E[Y] = aE[X] + b + E[\varepsilon]$

$E[Y] = aE[X] + b$

Variation of Y, i.e. second central moment:

$Var[Y] = Var[aX + b + \varepsilon]$

$Var[Y] = a^{2}Var[X] + Var[\varepsilon]$

$Var[Y] = a^{2}Var[X] + \sigma_{\varepsilon}^{2}$

Covariance of X & Y:

$\sigma_{XY} = aVar[X]$

#### 2. Using R, we can now solve for our parameters:

```{r q6-mom}
model1.a.est <- cov(model1.x, model1.y) / var(model1.x)
model1.b.est <- mean(model1.y) - (model1.a.est * mean(model1.x))
model1.sigma.eps.est <- sqrt(var(model1.y) - (model1.a.est^2 * var(model1.x)))

c(model1.a.est, model1.b.est, model1.sigma.eps.est)
```

### Reconciling Method of Moments with `lm()` function

***Reconcile our Method of Moments sigma estimate with our sigma obtained through lm().***

```{r q6-reconcile}
c(model1.summary$sigma^2, var(model1.summary$residuals), model1.sigma.eps.est^2)
model1.sigma.eps.est^2 * (sample.n - 1) / (sample.n - 2)

df.ratio <- (sample.n - 1) / (sample.n - 2)
c(model1.summary$sigma^2, 
  var(model1.summary$residuals) * df.ratio, 
  model1.sigma.eps.est^2 * df.ratio)
```

We can see that, since our Method of Moments estimate also uses `var()` and an N-1 denominator to calculate $\sigma_{\varepsilon}$ it is corrected with the same ration that we used above for `var(model1.summary$residuals)`.

Using a degrees of freedom ratio, we can reconcile all 3 derivations of $\sigma_{\varepsilon}$.

## 7. Fit lm() to the the Rest of Linear Models

Compare the differences between the assumptions of the 4 models and tell how they change the model behavior and estimated parameters.

```{r q7}
model2.est <- lm(Y~X, data = model2)
model2.summary <- summary(model2.est)
model3.est <- lm(Y~X, data = model3)
model3.summary <- summary(model3.est)
model4.est <- lm(Y~X, data = model4)
model4.summary <- summary(model4.est)
```

As we assess the model behavior and estimated parameters we obtained through lm(), we can consider the question we asked earlier in the Explanation section of Model 1:

Q. How does Y respond to predictor X?

### Model 1 - Normal Residuals

``` {r q7-m1}
model1.summary$coefficients
model1.summary$df
c(model1.summary$sigma, model1.summary$r.squared)

# Estimate r-squared from sample with Method of Moments
model1.rho.sq.est <- model1.a.est^2 * var(model1.x) / var(model1.y)
```

For this model the "true" answer to our question above, at least the one which we coded into our pseudo-random Model 1 sample is the following:

Y responds to predictor X in a linear fashion with some additional Gaussian randomness (residuals), according to the equation:

Predictor X:

$X \sim Norm(\mu=3, \sigma_{X}=2.5)$

Response Y:

$Y = aX + b + \varepsilon$

$\varepsilon \sim Norm(0, \sigma_{\varepsilon})$

Our "true" population parameters, which we used to build our sample, compared to the estimates produced by lm() and the Method of Moments are:

Parameter  | Theoretical | `lm()` Estimate | MoM Estimate
-----------|-------------|-----------------|--------------
a | `r a` | `r model1.summary$coefficients[[2]]` | `r model1.a.est`
b | `r b` | `r model1.summary$coefficients[[1]]` | `r model1.b.est`
$\sigma_{\varepsilon}$ | `r model1.eps.sd` | `r model1.summary$sigma` | `r model1.sigma.eps.est`
$\rho^2$ | `r rho.sq.theoretical` | `r model1.summary$r.squared` | `r model1.rho.sq.est`

Of all of our models, this model shows the strongest linear relationship between X nd Y and the highest $\rho^2$ value. The Gaussian residuals applied to Y, which randomly increase or decrease Y's values above or below the "perfect" linear line, keep Y centered around that line, varying about it in a normal relationship with most of the Y values within 1.5 of the line (1.5 being the standard deviation of our residuals). Thus, the positive linear relationship between X and Y is clearly demonstrated in both our plots and our calculated coefficient of determination.

For this example, we can also see that our estimated residual standard deviation based on the sample is quite close to our population or theoretical value, as is our r-squared.

### Model 2 - Uniform Residuals

``` {r q7-m2}
model2.summary$coefficients
model2.summary$df
c(model2.summary$sigma, model2.summary$r.squared)

# Calculate sigma epsilon through formula based on known uniform distrib parameters
model2.eps.sd <- sqrt((model2.max - model2.min)^2 / 12)
model2.rho.sq <- (a * model1.x.sd)^2 / ((a * model1.x.sd)^2 + model2.eps.sd^2)

# Estimate Model 2 parameters from sample using Method of Moments
model2.a.est <- cov(model1.x, model2.y) / var(model1.x)
model2.b.est <- mean(model2.y) - (model2.a.est * mean(model1.x))
model2.sigma.eps.est <- sqrt(var(model2.y) - (model2.a.est^2 * var(model1.x)))
model2.rho.sq.est <- model2.a.est^2 * var(model1.x) / var(model2.y)
```

For this model the "true" answer to our question, at least the one which we coded into our pseudo-random Model 2 sample is the following:

Y responds to predictor X in a linear fashion with some additional randomness (residuals) which is uniform between a specific interval, according to the equation:

Predictor X:

$X \sim Norm(\mu=3, \sigma_{X}=2.5)$

Response Y:

$Y = aX + b + \varepsilon$

$\varepsilon \sim Unif(`r model2.min`, `r model2.max`)$

Our "true" population parameters, which we used to build our sample, compared to the estimates produced by lm() and the Method of Moments are:

Parameter  | Theoretical | `lm()` Estimate | MoM Estimate
-----------|-------------|-----------------|--------------
a | `r a` | `r model2.summary$coefficients[[2]]` | `r model2.a.est`
b | `r b` | `r model2.summary$coefficients[[1]]` | `r model2.b.est`
$\sigma_{\varepsilon}$ | `r model2.eps.sd` | `r model2.summary$sigma` | `r model2.sigma.eps.est`
$\rho^2$ | `r model2.rho.sq` | `r model2.summary$r.squared` | `r model2.rho.sq.est`

For this model, the residuals exhibit a uniform rather than a normal distribution. When we know the minimum, a, and maximum, b, of a uniform distribution we can calculate its variance with the formula $\frac{(b-a)^2}{12}$. Using this formula we were able to calculate the residual standard deviation (above) and we see that it is larger than that of our 1.5 Model 1 $\sigma_{\varepsilon}$. With a larger variance of Y values, our $\rho^2$ coefficient of determination is smaller. As seen above this is true both of the value we calculated theoretically using $\sigma_{\varepsilon}$ and of the value estimated by `lm()`.

This makes sense. We still see a strong correlation but it is weaker than Model 1 since the residual is "more spread out". The residual from Model one was based on a normal distribution, therefore the resultant Y values, were largely packed very close to the correlated x,y line, as in a normal distribution most of the values (the top of the bell curve) are close to the mean. 

A unified distribution, however, does not have most of its values close to the mean, instead its values are evenly distributed. Therefore when our residual distribution was based on a unified distribution, the resultant Y values are evenly distributed away from the x,y line but still remain as close as the interval we specified, limiting their distance away.

Once again our estimated residual standard deviation ($\sigma_{\varepsilon}$ and r-squared $\rho^{2}$ based on the sample are quite close to our population or theoretical value.

### Model 3 - Cauchy Residuals

``` {r q7-m3}
model3.summary$coefficients
model3.summary$df
c(model3.summary$sigma, model3.summary$r.squared)

# Estimate Model 3 parameters from sample using Method of Moments
model3.a.est <- cov(model1.x, model3.y) / var(model1.x)
model3.b.est <- mean(model3.y) - (model3.a.est * mean(model1.x))
model3.sigma.eps.est <- sqrt(var(model3.y) - (model3.a.est^2 * var(model1.x)))
model3.rho.sq.est <- model3.a.est^2 * var(model1.x) / var(model3.y)
```

For this model the "true" answer to our question, at least the one which we coded into our pseudo-random Model 3 sample is the following:

Y responds to predictor X in a linear fashion with some additional Cauchy randomness (residuals) which may in some ways appear Gaussian yet includes wildly divergent outliers, according to the equation:

Predictor X:

$X \sim Norm(\mu=3, \sigma_{X}=2.5)$

Response Y:

$Y = aX + b + \varepsilon$

$\varepsilon \sim Cauchy(location=0, scale=3)$

Our "true" population parameters, which we used to build our sample, compared to the estimates produced by lm() and the Method of Moments are:

Parameter  | Theoretical | `lm()` Estimate | MoM Estimate
-----------|-------------|-----------------|--------------
a | `r a` | `r model3.summary$coefficients[[2]]` | `r model3.a.est`
b | `r b` | `r model3.summary$coefficients[[1]]` | `r model3.b.est`
$\sigma_{\varepsilon}$ | n/a | `r model3.summary$sigma` | `r model3.sigma.eps.est`
$\rho^2$ | n/a | `r model3.summary$r.squared` | `r model3.rho.sq.est`

This model represents our most extreme example of widely varied residuals reducing the strength of the linear relationship between our variables. The effect of a Cauchy distribution on our $\varepsilon$ results in a residual standard deviation of ~19 and reduces our coefficient of determination to less than 1/100.

Comparing the Model 3 plot with the Model 3 zoomed-in plot above, it is important to note however that the central Y values still have a strong relationship with X, yet the Y outliers are so extreme that a chart displaying all Y values seems to entirely obfuscate the relationship. The residual outliers radically increase the overall spread of the Y values.

This shows us that Cauchy distributions certainly obfuscates the nature of our data especially when using estimated parameters to assess correlation. Seeking a solution to this problem leads us to recall the concept of the *trimmed mean* which, though somewhat uncommon, may be used with extremely skewed distributions to calculate a mean which excludes "misleading" outliers in order to get a better sense of the behavior of the center of the data. We will see in the future how our knowledge of the Cauchy distribution and/or data cleansing can be used to find relationships when their existence is skewed by outliers which may represent measurement or user error.

Finally it is important to note that our estimated residual standard deviation ($\sigma_{\varepsilon}$ and r-squared $\rho^{2}$ based on the sample are but "chimeras" in the Cauchy example. Though it is indeed possible to calculate these figures using the data in the sample we are given, we note that were we to collect multiple additional samples from our population, we would not find that the set of estimated parameters based on our samples bore any relationship to eachother or a theoretical "correct" residual standard deviation or coefficient of determination of our population. This, again, is due to the obfuscating nature of the Cauchy family of distributions and the fact that there is no true mean or standard distribution that may be calculated to describe full set of Cauchy data.

### Model 4 - Heteroscedastic Residuals

``` {r q7-m4}
model4.summary$coefficients
model4.summary$df
c(model4.summary$sigma, model4.summary$r.squared)

# Estimate Model 4 parameters from sample using Method of Moments
model4.a.est <- cov(model1.x, model4.y) / var(model1.x)
model4.b.est <- mean(model4.y) - (model4.a.est * mean(model1.x))
model4.sigma.eps.est <- sqrt(var(model4.y) - (model4.a.est^2 * var(model1.x)))
model4.rho.sq.est <- model4.a.est^2 * var(model1.x) / var(model4.y)
```

For this model the "true" answer to our question, at least the one which we coded into our pseudo-random Model 4 sample is the following:

Y responds to predictor X in a linear fashion with some additional heteroscedastic randomness (residuals) which may in some ways appear Gaussian yet due to an inconsistent standard deviation in the data exhibits greater kurtosis than a Gaussian distribution, according to the equation:

Predictor X:

$X \sim Norm(\mu=3, \sigma_{X}=2.5)$

Response Y:

$Y = aX + b + \varepsilon$

$\varepsilon \sim$ multiple sub-sequences on $Norm(0, \sigma_{\varepsilon_{1-4}})$

Our "true" population parameters, which we used to build our sample, compared to the estimates produced by lm() and the Method of Moments are:

Parameter  | Theoretical | `lm()` Estimate | MoM Estimate
-----------|-------------|-----------------|--------------
a | `r a` | `r model4.summary$coefficients[[2]]` | `r model4.a.est`
b | `r b` | `r model4.summary$coefficients[[1]]` | `r model4.b.est`
$\sigma_{\varepsilon}$ | various; mean of `r mean(hetero.process)` | `r model4.summary$sigma` | `r model4.sigma.eps.est`
$\rho^2$ | n/a | `r model4.summary$r.squared` | `r model4.rho.sq.est`

Due to heteroscedasticiy of this model, it's important to note that we cannot adequately describe the one theoretical standard deviation or coefficient of determination of the data, since the data represents a population which is compiled from what is essentially varying sub-populations each with their own Gaussian behavior on a different standard deviation.

Accordingly, when we observe our plot of X-Y data, the fact that the residuals are not normal is not visually apparent (at least not to the untrained eye). Our Model 4 plot is strikingly similar to our Model 1 plot, to the extent that it could be mistaken for a normal residual with simply a greater standard deviation. Our alternate heteroscedastic Model 5 with a closer mean standard deviation is even more similar to Model 1.

As we demonstrated with Model 5 earlier, the fact that Model 6 has a lower correlation and coefficient of determination than Model 1 is not due to its heteroscedasticity but due to the greater overall standard deviation of its sub-populations. We will demonstrate this further below.

What these plots appear to demonstrate is that, while our Cauchy distribution exhibits a "hidden linear relationship" that we may not grasp from our plot due to its radical outliers, our heteroscedastic distribution plays a different trick on us. Rather than *not* seeing something that is there under the hood as with the Cauchy, we are *seeing something* that isn't really there, or at least not as we perceive it. Our heteroscedastic distribution may seem at first glance to demonstrate a simple Gaussian residual, but in fact its residual is more complex.

Only by analyzing the distribution of the residul itself do we see this effect. Both our dot plot of the Model 4 residual and our density graph show something distinctly non-normal. As noted above, the distribution of our heteroscedastic distribution is leptokurtic, that is, shows greater kurtosis. 

Here we should recognize that the important feature of kurtosis is not its increased peakedness but its heavier tail. Higher kurtosis means that more of the variance is the result of infrequent extreme deviations, as opposed to frequent modestly sized deviations. 

Let's tease out what this means and how it behaves. As with our Cauchy distribution, it is possible to calculate a single standard deviation from our data, but this number does not represent how *all of the population* behaves, as it would with a more consistent data set. Because our population is actually a group of sub-populations each with a different distribution or "rule" governing their spread, estimating a parameter for the entire population and using that number to predict the behavior of the population will not be wholly accurate. The portions of the data which in fact have a higher standard deviaion will produce values with greater variance than we would expect, and vice versa.

As a final demonstration of this behavior, we can show that no matter which direction or amount we vary our sub-populations' standard deviations, the result is a leptokurtic residual distribution. We also see that calculating the correlation between X and Y tells us little about the existence of a heteroscedastic distribution. This behavior is more intuitively seen through the following plots.

An interesting observation of the impact of the leptokurtic residuals is that our lm() function predicted a residual standard deviation that is notably higher than the mean standard deviation of the heteroscedastic process that produced the data sample. We can suspect this may due to the effect of the heavy tail of our leptokurtic residual.

```{r q7-m6, fig.width=3.75, fig.height=3.25}
# Create a function to plot our theoretical vs. heteroscedastic sample
PlotHetero <- function (model.hetero) {
  plot(model.hetero)
  
  hetero.x <- (100 * floor(min(model.hetero))) : (100 * ceiling(max(model.hetero)))
  hetero.x <- hetero.x / 100
  
  # Plot the theoretical distribution
  plot(hetero.x, dnorm(hetero.x, mean = mean(model.hetero), sd = sd(model.hetero)), 
       type = "l", ylim = c(0, 0.6), col = "black", 
       ylab = "Distribution of Heteroscedastic Eps")
  
  # And the heteroscedastic sample distribution
  lines(density(model.hetero), col = "Red")
}

# Create function to produce heteroscedastic sample
ModelHetero <- function (hetero.sd) {
  # Create heteroscedastic process
  hetero.loop <- c(rep(hetero.sd[1], 50),
                   rep(hetero.sd[2], 75),
                   rep(hetero.sd[3], 75),
                   rep(hetero.sd[4], 50))
  hetero.process <- rep(hetero.loop, 4)

  # Simulate the linear model residuals Eps with changing standard deviations.
  set.seed(seed2)
  model.hetero.eps <- rnorm(sample.n) * hetero.process
  
  # Simulate the model using X from Model 1 and heteroscedastic residuals
  model.hetero.y <- (a * model1.x + b) + model.hetero.eps
  return(data.frame(Y=model.hetero.y, X=model1.x, Eps=model.hetero.eps))
}

model.hetero <- ModelHetero(c(9, 1.5, 1.7, 1.6))
cor(model.hetero$X, model.hetero$Y)
PlotHetero(model.hetero$Eps)

model.hetero <- ModelHetero(c(0.2, 1.5, 1.7, 1.6))
cor(model.hetero$X, model.hetero$Y)
PlotHetero(model.hetero$Eps)

model.hetero <- ModelHetero(c(2, 2, 0.5, 0.5))
cor(model.hetero$X, model.hetero$Y)
PlotHetero(model.hetero$Eps)

model.hetero <- ModelHetero(c(1.6, 1.6, 1.7, 1.7))
cor(model.hetero$X, model.hetero$Y)
PlotHetero(model.hetero$Eps)
```